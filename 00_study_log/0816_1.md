# LogStash

> 무료 개방형 서버의 데이터 처리 파이프라인인 
> Logstash는 다양한 소스에서 데이터를 수집하여 변환한 후 
> 자주 사용하는 저장소로 전달합니다.

## Logstash의 구성요소

**입력, 필터, 출력**
Logstash는 형식이나 복잡성과 관계 없이 데이터를 동적으로 수집, 전환, 전송한다.

**grok**을 이용해 비구조적 데이터에서 구조를 도출하여 
- IP 주소에서 위치 정보 좌표를 해독하고, 
- 민감한 필드를 익명화하거나 제외시키며, 
- 전반적인 처리를 손쉽게 수행할 수 있다.

**INPUT**
- 모든 형태, 크기, 소스의 데이터 수집

- 데이터는 여러 시스템에 다양한 형태로 보관된 경우가 많다. 
- Logstash는 일반적인 다수의 소스에서 동시에 이벤트를 가져오는 다양한 입력을 지원한다.
- 로그, 메트릭, 웹 애플리케이션, 데이터 저장소 등, 모두 지속적으로 스트리밍되는 방식으로 손쉽게 수집할 수 있다.

**FILTER**
Logstash 필터는 데이터가 소스에서 저장소로 이동하는 과정에서 각 이벤트를 구문 분석하고 명명된 필드를 식별하여 구조를 구축하며, 이를 공통 형식으로 변환 통합한다.

- 형식이나 복잡성에 관계없이 다음과 같이 데이터를 동적으로 변환하고 준비한다.
  - grok을 통해 비정형 데이터에서 구조 도출
  - IP 주소에서 위치 좌표 해독
  - PII 데이터의 익명화, 민감한 정보 필드 완전 제외
  - 데이터 소스나 형태, 스키마에  상관없이  전체적으로 손쉬운 처리

**OUTPUT**
- 스태시를 선택하여 데이터 전송
- 원하는 곳으로 데이터를 라우팅할 수 있는 다양한 출력을 지원하기 때문에 여러 저장소로 데이터를 다운스트림하는 유연성을 확보할 수 있다.

## Log 분석의 필요성

**MSA :: Microservice Architecture**
- MSA는 서비스를 작은 단위로 나누어 개발하고, 각 서비스를 독립적으로 배포하고 실행하는 아키텍처 패턴이다.
- 각 서비스는 독립적으로 배포되기 때문에, 서비스 간의 통신이 필요하고, 이를 위해 로그를 분석하는 것이 필요하다.   
  - 기존에는 1개의 로그가 발생하는 이벤트도, MSA에서는 여러 서비스에서 발생할 수 있다.
  - n개의 서비스로 나뉜다면 n개의 로그가 발생할 수 있다.
- 이러한 이유로 로그 분석은 MSA에서 더욱 중요해졌다.

Logstash는 서로 다른 소스의 데이터를 탄력적으로 통합하고 사용자가 선택한 목적지로 데이터를 정규화할 수 있습니다.

## Grok 패턴

> Grok은 Logstash에서 사용하는 강력한 패턴 엔진이다.

- Grok은 로그 데이터를 구문 분석하고, 필드를 추출하는 데 사용되는 패턴을 정의한다.
- Grok 패턴은 %{SYNTAX:SEMANTIC} 형식으로 구성되며, SYNTAX는 정규표현식을 사용하여 SEMANTIC을 추출한다.
  - SYNTAX: 추출하고자 하는 데이터의 패턴
  - SEMANTIC: 추출한 데이터의 이름
  - 예) %{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes}
  - IP, WORD, URIPATHPARAM, NUMBER는 Grok 패턴이다.
- Grok 패턴은 Logstash에서 제공하는 기본 패턴 외에도 사용자가 직접 정의할 수 있다.
- Grok 패턴을 사용하면 비구조적인 데이터를 구조화하여 분석하기 쉽게 만들 수 있다.

```ruby
filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
}
```

## Logstash에서의 Grok 패턴 사용

- Logstash에서 Grok 패턴을 사용하려면 filter 블록 내에 grok 블록을 추가하고, match 옵션에 Grok 패턴을 지정하면 된다.

```ruby
input {
  stdin { }
}

filter {
  mutate {
    add_field => { "message" => "this is a test!" }
  }
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
}

output {
  stdout { codec => rubydebug }
}
```

- 이때 `input` 블록에서는 데이터를 입력받는 방법을 지정하고, 
- `filter` 블록에서는 데이터를 가공하는 방법을 지정한다.
- `output` 블록에서는 가공된 데이터를 출력하는 방법을 지정한다.

`input` 과 `output` 블록은 필수이며, `filter` 블록은 선택사항이다.

```ruby

input {
    stdin {}
}

filter {
    mutate {
        add_field => { "description" => "second pipeline!" }
    }
}

output {
    elasticsearch {
	    hosts => "127.0.0.1:9200"
	    index => "logstash-%{+YYYY.MM.dd}"
			data_stream => false
  }
}
```

- 이렇게 작성된 Logstash 설정 파일을 실행하면,
- `stdin`으로 입력받은 데이터를 가공한 후, `elasticsearch`로 출력한다.
- 이때 `elasticsearch`로 출력할 때는 `hosts`와 `index`를 지정해야 한다.
  - `hosts`: Elasticsearch 호스트의 주소와 포트를 지정한다.
  - `index`: Elasticsearch에 저장될 인덱스의 이름을 지정한다.
  - `data_stream`: 데이터 스트림을 사용할지 여부를 지정한다. (기본값은 `false`)

## Logstash의 장점

- 다양한 소스에서 데이터를 수집하여 변환한 후 자주 사용하는 저장소로 전달할 수 있다.
- Logstash는 형식이나 복잡성과 관계 없이 데이터를 동적으로 수집, 전환, 전송한다.
- grok을 이용해 비구조적 데이터에서 구조를 도출하여 IP 주소에서 위치 정보 좌표를 해독하고, 민감한 필드를 익명화하거나 제외시키며, 전반적인 처리를 손쉽게 수행할 수 있다.

## Beats 

- Beats는 경량 데이터 수집기로, 다양한 소스에서 데이터를 수집하여 Logstash 또는 Elasticsearch로 전송한다.
  - Filebeat: 로그 파일을 수집하는 데이터 수집기
  - Metricbeat: 시스템 및 서비스의 메트릭을 수집하는 데이터 수집기
  - Packetbeat: 네트워크 패킷을 수집하는 데이터 수집기
  - Winlogbeat: Windows 이벤트 로그를 수집하는 데이터 수집기
  - ... 등, 다양한 데이터 수집기가 제공된다.

```ruby
# logstash-pipeline-3.conf 

input {
    beats {
        port => 5044
    }
}

filter {
    grok {
        match => { "message" => "%{COMBINEDAPACHELOG}"}
    }

    mutate {
        remove_field => ["host", "@version", "message"]
    }
}

output {
  elasticsearch {
    hosts => "127.0.0.1:9200"
    index => "logs-%{+YYYY.MM.dd}"
  }
}
```

위 설정 파일은 Beats로 수집한 데이터를 가공한 후 Elasticsearch로 전송하는 설정 파일이다.

- `input` 블록에서는 Beats로 수집한 데이터를 입력받는다.
  - `beats` 플러그인을 사용하며, `port` 옵션으로 포트를 지정한다.
  - Beats는 기본적으로 5044 포트를 사용하므로, `port` 옵션을 생략하면 5044 포트로 설정된다.
- `filter` 블록에서는 데이터를 가공한다.
  - `grok` 플러그인을 사용하여 데이터를 가공한다.
  - `mutate` 플러그인을 사용하여 필드를 추가하거나 제거한다.
  - 이때 `remove_field` 옵션을 사용하여 필드를 제거할 수 있다.
- `output` 블록에서는 가공된 데이터를 출력한다.
  - `elasticsearch` 플러그인을 사용하여 Elasticsearch로 데이터를 전송한다.
  - `hosts`와 `index`를 지정하여 Elasticsearch 호스트와 인덱스를 지정한다.
    - `hosts`: Elasticsearch 호스트의 주소와 포트를 지정한다.
    - `index`: Elasticsearch에 저장될 인덱스의 이름을 지정한다.

